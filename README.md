# ğŸ¥ğŸ” video_content_search

**video_content_search** is a multimodal search library that enables searching inside video content using **text**, **image**, or **audio** queries â€” and returns semantically relevant results from the transcript, screenshots, and soundbites of the video.

---

It includes:

* A data pipeline to prepare video modalities (images, text, audio) for insertion into a vector store
* A vector database interface for Weaviate (`WeviateMultimodal`)
* Support for multimodal similarity search across text, image, and audio using ImageBind
* A Gradio-powered web app for demoing and exploring search results

---

Let me know if you want it more formal, concise, or detailed!

---

## âœ¨ Features

- Parsing video into modalities (image, audio and text) and preparing each for vector store
- ğŸ”¤ Text-based semantic search across video transcripts
- ğŸ–¼ï¸ Image-based search for matching video frames
- ğŸ”Š Audio-based search for retrieving soundbites
- â±ï¸ Returns video ID and timestamp for all results
- ğŸ§  Multimodal embedding and retrieval using Weaviate vector DB

---

## ğŸš€ Demo App

### ğŸ”§ Prerequisites

- Python 3.9+
- Poetry
- Weaviate running in docker
- Imagebind running in docker
- Whisper
- Gradio


â–¶ï¸ Run the Gradio App

First, make sure the video_content_search package is available via Poetry.

âœ… 1. Install Dependencies
bash
Copy
Edit
poetry install
This installs both the main package (video_content_search) and the Gradio app dependencies defined in pyproject.toml.

ğŸš€ 2. Launch the App
bash
Copy
Edit
poetry run python app/gradio_app.py
This will start the Gradio web interface where you can search using text, image, or audio, and view results retrieved from your Weaviate multimodal collection.



This launches a Gradio web UI where you can:

1. Enter text
2. Upload an image
3. Record or upload audio and
4. get matching text/image/audio chunks from the video with their video_id and timestamp.


ğŸ§  How It Works
When a user submits a query:

* The app encodes it based on its modality (text/image/audio)
* Queries Weaviate for top-k similar results across all 3 modalities
* Decodes results (e.g., base64 images, audio bytes)
* Displays them in Gradio, along with their metadata

Each result includes:
- Semantic similarity score 
- Video ID 
- Timestamp in the video



